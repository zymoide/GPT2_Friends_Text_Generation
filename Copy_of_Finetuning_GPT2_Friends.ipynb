{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Finetuning_GPT2_Friends.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN3QFCLZl749LCN4am5xPri",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zymoide/GPT2_Friends_Text_Generation/blob/main/Copy_of_Finetuning_GPT2_Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxeFueoOt7yX",
        "outputId": "589b2962-5f39-49f4-886a-569d70520c0d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install gpt-2-simple\n",
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c9/44fe420225244ab9e3f2938a1d11651681078cf72f7444c66d0ea49f1320/gpt_2_simple-0.7.2.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.2-cp37-none-any.whl size=23621 sha256=2d1db735e12fd77aeb2da54a339739bc0157830c9043991b229aefb4ef72790d\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/1d/15/c87a4302a6c7273ce1b4f282bec3c6877fb2f521535d87d30f\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.2 toposort-1.6\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-6sFVmyVjyf",
        "outputId": "87fdc42b-8e10-4f8b-b593-3c4ef0173c2d"
      },
      "source": [
        "#trying to load the saved model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfNGaNI9VzpH"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icmnLVXVYw-M"
      },
      "source": [
        "#!cp -r /content/drive/MyDrive/GPT2_Project/checkpoint/run1* /content/drive/MyDrive/GPT2_Project/models/124M\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "TLc-3h3pWwNF",
        "outputId": "65fa973f-c9a1-4c1a-b02b-5eb683a82e5b"
      },
      "source": [
        "gpt2.load_gpt2(sess,checkpoint_dir='/content/drive/MyDrive/GPT2_Project/checkpoint',model_dir='/content/drive/MyDrive/GPT2_Project/models')\n",
        "\n",
        "#def load_gpt2(sess, checkpoint='latest', run_name='run1', checkpoint_dir='checkpoint', model_name=None, model_dir='models', multi_gpu=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d20e2d28059d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GPT2_Project/checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GPT2_Project/models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#def load_gpt2(sess, checkpoint='latest', run_name='run1', checkpoint_dir='checkpoint', model_name=None, model_dir='models', multi_gpu=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, checkpoint, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'latest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoaYqMsHqcyR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpfdWUb2t986",
        "outputId": "ca398970-7fca-4bb1-9719-d56acf0b01d7"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 517Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 4.86Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 636Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 28.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 580Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.14Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 8.44Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TdONXrguFfo"
      },
      "source": [
        "file_name = '/content/drive/MyDrive/GPT2_Project/Friends_Transcript.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XkD2aEHYwYPo",
        "outputId": "7f43ca4f-f7ce-4117-b4eb-34ed1dc82528"
      },
      "source": [
        "file_name"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/GPT2_Project/Friends_Transcript.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgelFw-JukcC",
        "outputId": "44201fc4-9bda-4911-a515-835ab33a8b6a"
      },
      "source": [
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              model_dir='/content/drive/MyDrive/GPT2_Project/models',\n",
        "              steps=1500,\n",
        "              restore_from='latest',\n",
        "              run_name='run1',\n",
        "              print_every=200,\n",
        "              checkpoint_dir='/content/drive/MyDrive/GPT2_Project/checkpoint',\n",
        "              sample_every=500,\n",
        "              save_every=200,\n",
        "              overwrite=True\n",
        "              )\n",
        "\n",
        "# def finetune(sess, dataset, steps=-1, model_name='124M', model_dir='models', combine=50000, batch_size=1, \n",
        "#              learning_rate=0.0001, accumulate_gradients=5, restore_from='latest', run_name='run1',\n",
        "#              checkpoint_dir='checkpoint', sample_every=100, sample_length=1023, sample_num=1, multi_gpu=False, \n",
        "#              save_every=1000, print_every=1, max_checkpoints=1, use_memory_saving_gradients=False, \n",
        "#              only_train_transformer_layers=False, optimizer='adam', overwrite=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint /content/drive/MyDrive/GPT2_Project/checkpoint/run1/model-1500\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/GPT2_Project/checkpoint/run1/model-1500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.56s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1428868 tokens\n",
            "Training...\n",
            "Saving /content/drive/MyDrive/GPT2_Project/checkpoint/run1/model-1500\n",
            "======== SAMPLE 1 ========\n",
            " late and we need to talk.\n",
            "Phoebe: Ok. I mean Im not saying hes wrong, but I have this feeling. I mean maybe Im gonna want to do something now, now. Maybe I am just not ready yet. I mean its probably not gonna be a big deal to you at all. (to Monica) Hey, Monica!\n",
            "Monica: Hey, Phoebe! Oh my god, this is the best!\n",
            "Phoebe: Yes!\n",
            "Monica: (whispering) What, exactly am I gonna get?\n",
            "Phoebe: Well nothing, everything! Youre gonna cry! I know youre gonna cry, but the reason that we are in this together is so that we could all get a baby!\n",
            "Monica: Wow! Well then maybe you wont cry when its already a baby.\n",
            "Phoebe: Yes, but thats totally fine with me because this is going to be my baby and my place to always be. Now dont worry about me, okay? I am gonna cry! And thats gonna be really sweet. (Shes interrupted by the crying voice in the background, which Phoebe recognizes) Ohh Im going to have my lips! (The baby starts crying)\n",
            "(Rachel slowly comes out of her room.)\n",
            "Rachel: Ohh! Thats-thats adorable! Oh, and there is no baby! (Monica is in tears)\n",
            "Monica: Oh God!\n",
            "Rachel: Well, we are not gonna be able to make it happen in ten minutes.\n",
            "Monica: Ok. (She hugs Monica.)\n",
            "[Scene: The delivery room. Ross is at the delivery room crying.]\n",
            "Ross: I cant!! The doctor says its just gonna be a wait, one person to four people. Im trying! (He starts to walk Ross towards something.) Hey-hey, whats in it?\n",
            "(The doctor starts to administer the boy/girl pill, but Ross stops it and takes a big swig of his coffee.)\n",
            "Ross: Its just a bottle.\n",
            "Dr. Long: (closing the door) Ross! I am so sorry that we are gonna have to delay your baby. I cannot tell you how sorry I got when you were just five days overdue. Oh, and I bet that, by now, youre probably feeling more at home on the couch.\n",
            "Ross: (starting to cry) Oh! (Looking at the baby, who nods in agreement with him.) Ohh! And-and its so beautiful! Oh! And, thank God your Mom is here!\n",
            "Dr. Long: (breaking the door) Ross-donal do look, its for your internal corsaometer. (Dr. Ross stares at him in shock.)\n",
            "Ross: Ah thats right. No! No!\n",
            "Dr. Long: Well can I give you a second to get it off? Youre done, its-its-its gonna take a while, so\n",
            "Ross: Uh, well see. (The doctor hands him the baby.)\n",
            "Dr. Long: Its a girl, named Rosita.\n",
            "Ross: Oh!\n",
            "Rachel: (breaking the door) Ross! We still have to go out to dinner! (Ross looks at her, shocked, in a hurry.)\n",
            "[Scene: Joey and Rachel's apartment. Joey is sitting on the couch as Phoebe enters with the chick and duck.]\n",
            "Phoebe: Hey!\n",
            "Joey: Hey! (Phoebe is hopping on the couch to look down the hall.)\n",
            "Phoebe: Oh my god! How was your date?\n",
            "Joey: Oh, it was so awkward! Chandler came by to grab some peanuts and duck pooped in his lap. Joey, this is your apartment!\n",
            "Phoebe: No! (Points to the guy sitting there.)\n",
            "Joey: Look! You promised me a little commitment while we were going out! Im almost married! Look, promise is a tricky thing to make but, this is like yesterday! Oh! I went to a prom with my roommate! And there was a party!\n",
            "Phoebe: Oh thats what it is. That is how you get caught up when youre engaged!\n",
            "Joey: Hey! Well look, promise says \"Days From Monday,\" so the thing is like yesterday. (They look around the apartment.)\n",
            "Rachel: Hey! Joey, I need to talk to you about this party.\n",
            "Joey: I know! I-I need to talk to you about this whole \"We dont\" thing.\n",
            "Rachel: Well you dont have to put all your eggs in one basket, okay? Everything is gonna fall into place, and then some, you name it. And then after you get started, you may as well put it together.\n",
            "Joey: I dont care! All right, look-look, look whats this party about? Wh(Raises his hand)This is about the fact that my\n",
            "\n",
            "[1600 | 1820.91] loss=1.95 avg=1.95\n",
            "Saving /content/drive/MyDrive/GPT2_Project/checkpoint/run1/model-1600\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Hh7GsrwW0a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OSLboKEuoqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15893503-78c3-44ee-e22e-b24cbd80e05a"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              checkpoint_dir = '/content/drive/MyDrive/GPT2_Project/checkpoint',\n",
        "              model_dir='/content/drive/MyDrive/GPT2_Project/models',\n",
        "              #return_as_list=True,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Mona broke up with Ross.\",\n",
        "              nsamples=1,\n",
        "              batch_size=1,\n",
        "              top_k=40\n",
        ")\n",
        "\n",
        "\n",
        "# def generate(sess, run_name='run1', checkpoint_dir='checkpoint', model_name=None, model_dir='models', \n",
        "#              sample_dir='samples', return_as_list=False,\n",
        "#              truncate=None, destination_path=None, sample_delim='=' * 20 + '\\n',\n",
        "#              prefix=None, seed=None, nsamples=1, batch_size=1, length=1023, temperature=0.7,\n",
        "#              top_k=0, top_p=0.0, include_prefix=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mona broke up with Ross. I mean, that's crazy! I have no idea what you did, Ross, what you did, that's crazy!\n",
            "Ross: It's not crazy.\n",
            "Rachel: What? What?!\n",
            "Ross: Okay, okay, okay, okay, okay, I'm sorry, I'm, I'm, I'm sorry, I, I'm, I'm, I'm sorry, I, I think it's a little crazy, but ummI'll make it so you know, it's not crazy, it's justI mean, it's just, it's, it's just been so long since I've been with somebody, and I just, ummOh! So, I'm, I'm gonna, I'm gonna, I'm gonna tell you what, you guys meet me at the coffee house, I'm gonna go to the bathroom, and I'm gonna, I'm gonna, I'm gonna tell you what. You guys meet me at the coffee house, I'm gonna go to the bathroom, and I'm, I'm gonna, I'm gonna tell you what.\n",
            "Monica: Well, I mean, you're, you're gonna tell me what.\n",
            "Ross: Oh yeah, okay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQFMz2LIIgPu"
      },
      "source": [
        "generated_script = gpt2.generate(sess,\n",
        "              length=350,\n",
        "              temperature=0.9,\n",
        "              prefix=\"Chandler and Monica finally decided to leave New York\",\n",
        "              nsamples=1,\n",
        "              batch_size=1,\n",
        "              top_k=30,\n",
        "              return_as_list=True\n",
        ")[0]\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETiijaGTTyAu",
        "outputId": "47aadfdd-34eb-4023-8bb8-af651c170613"
      },
      "source": [
        "type(generated_script)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ0kaGFHHyR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1032f3c8-2f72-43be-a124-fa02ef8fe3da"
      },
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 28.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9MB 25.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 962kB 40.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
            "\u001b[?25h  Building wheel for flask-cachebuster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 6.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 41.1MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-zRjb-2K6IT"
      },
      "source": [
        "def generating_text(initial_text):\n",
        "    return gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix= initial_text,\n",
        "              nsamples=1,\n",
        "              batch_size=1,\n",
        "              top_k=40,\n",
        "              return_as_list=True\n",
        ")[0]\n",
        "   \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "W3C6-dRZE0Zl",
        "outputId": "f597a64d-c802-4b7a-8204-44140f85e779"
      },
      "source": [
        "def greet(name):\n",
        "    return \"Hello, my name is \" +  name\n",
        "\n",
        "iface = gr.Interface(greet,\"textbox\",\"textbox\")    \n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://58498.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"500\"\n",
              "            src=\"https://58498.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f2db12f8e50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7873/',\n",
              " 'https://58498.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onbFU9oZ7Mpd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FkZsEiPJpg-"
      },
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "8VGOpm20KQAW",
        "outputId": "a816810d-e09e-47e0-8517-f2111e01a0be"
      },
      "source": [
        "output_text = gr.outputs.Textbox()\n",
        "#gr.Interface(generating_text,\"textbox\", output_text, title=\"GPT2 Finetune\",\n",
        "             #description=\"I finetuned GPT-2 on Friends script.\").launch()\n",
        "gr.Interface(generating_text,\"text\", \"text\",capture_session=True).launch()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://39292.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"500\"\n",
              "            src=\"https://39292.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f58c6282ed0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7861/',\n",
              " 'https://39292.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1ojigfLsXW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnjGMmpI7OEh",
        "outputId": "7f19e9e6-acb4-4646-9bf3-591cb24a0c37"
      },
      "source": [
        "!zip -r /content/file.zip /content/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: content/models/ (stored 0%)\n",
            "updating: content/models/124M/ (stored 0%)\n",
            "updating: content/models/124M/hparams.json (deflated 28%)\n",
            "updating: content/models/124M/model.ckpt.index (deflated 62%)\n",
            "updating: content/models/124M/checkpoint (deflated 42%)\n",
            "updating: content/models/124M/vocab.bpe (deflated 53%)\n",
            "updating: content/models/124M/model.ckpt.data-00000-of-00001 (deflated 7%)\n",
            "updating: content/models/124M/encoder.json (deflated 67%)\n",
            "updating: content/models/124M/model.ckpt.meta (deflated 91%)\n",
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 23%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2021.05.06/ (stored 0%)\n",
            "  adding: content/.config/logs/2021.05.06/13.44.00.991142.log (deflated 54%)\n",
            "  adding: content/.config/logs/2021.05.06/13.43.23.909017.log (deflated 54%)\n",
            "  adding: content/.config/logs/2021.05.06/13.44.01.543195.log (deflated 53%)\n",
            "  adding: content/.config/logs/2021.05.06/13.43.04.692209.log (deflated 91%)\n",
            "  adding: content/.config/logs/2021.05.06/13.43.39.026092.log (deflated 86%)\n",
            "  adding: content/.config/logs/2021.05.06/13.43.44.620859.log (deflated 53%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/Friends_Transcript.txt (deflated 65%)\n",
            "  adding: content/checkpoint/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/ (stored 0%)\n",
            "  adding: content/checkpoint/run1/hparams.json (deflated 28%)\n",
            "  adding: content/checkpoint/run1/checkpoint (deflated 40%)\n",
            "  adding: content/checkpoint/run1/vocab.bpe (deflated 53%)\n",
            "  adding: content/checkpoint/run1/events.out.tfevents.1620639864.7dcee0992def (deflated 60%)\n",
            "  adding: content/checkpoint/run1/encoder.json (deflated 67%)\n",
            "  adding: content/checkpoint/run1/model-1500.index (deflated 62%)\n",
            "  adding: content/checkpoint/run1/model-1500.meta (deflated 91%)\n",
            "  adding: content/checkpoint/run1/model-1500.data-00000-of-00001 (deflated 7%)\n",
            "  adding: content/checkpoint/run1/counter (stored 0%)\n",
            "  adding: content/samples/ (stored 0%)\n",
            "  adding: content/samples/run1/ (stored 0%)\n",
            "  adding: content/samples/run1/samples-601 (deflated 58%)\n",
            "  adding: content/samples/run1/samples-801 (deflated 55%)\n",
            "  adding: content/samples/run1/samples-1201 (deflated 54%)\n",
            "  adding: content/samples/run1/samples-1001 (deflated 56%)\n",
            "  adding: content/samples/run1/samples-201 (deflated 57%)\n",
            "  adding: content/samples/run1/samples-1401 (deflated 53%)\n",
            "  adding: content/samples/run1/samples-401 (deflated 56%)\n",
            "  adding: content/flagged/ (stored 0%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 42%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6omsGzbD52IL"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dB8gYJAB7t5u",
        "outputId": "0dadb50a-ab7c-4b5b-d714-e3c8ab39be33"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e991501d-c0e5-4a87-a740-94631b399dd0\", \"file.zip\", 935045747)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v8Li3WZ7weL",
        "outputId": "37502608-f0a8-4929-9df5-da1e80a168ef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBIwBGatC46g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}